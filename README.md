# AquaLLM - Sistema de AtenciÃ³n al Cliente con IA

Un sistema de chatbot inteligente diseÃ±ado para que las empresas de agua potable ofrezcan atenciÃ³n al cliente 24/7. El chatbot puede responder preguntas sobre facturaciÃ³n, consumo, contratos y solicitudes utilizando una base de datos de clientes y un modelo de lenguaje local (Ollama).
---

## ğŸš€ TecnologÃ­as Utilizadas

-   **Frontend:** React
-   **Backend:** Python con FastAPI
-   **Base de Datos:** Supabase (PostgreSQL)
-   **IA / LLM:** Ollama (ejecutando modelos como `gemma:2b` o `tinyllama` localmente)
-   **Servidor:** Uvicorn

---

## âœ¨ CaracterÃ­sticas

-   **Consulta de Datos:** El usuario puede preguntar usando su nÃºmero de cliente o medidor.
-   **Respuestas Contextualizadas:** El sistema busca informaciÃ³n real del cliente en la base de datos (facturas, consumos, estado del servicio) para generar respuestas precisas.
-   **Procesamiento de Lenguaje Natural:** Utiliza un LLM para entender la pregunta del usuario y generar una respuesta en lenguaje natural.
-   **OperaciÃ³n Local:** Funciona de forma 100% local (despuÃ©s de la configuraciÃ³n inicial), sin depender de APIs de terceros para la IA.

---

## ğŸ“‹ Prerrequisitos

Antes de empezar, asegÃºrate de tener instalado lo siguiente:

-   [Node.js](https://nodejs.org/en/) (versiÃ³n LTS recomendada)
-   [Python](https://www.python.org/downloads/) (versiÃ³n 3.8 o superior)
-   [Ollama](https://ollama.com/) y un modelo descargado (ej. `ollama run gemma:2b`)

---

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

Sigue estos pasos para poner en marcha el proyecto en tu mÃ¡quina local.

### 1. Clonar el Repositorio

```bash
git clone https://github.com/Tokioh/AquaLLM.git
cd AquaLLM
```

### 2. Configurar el Backend

```bash
# 1. Navega a la carpeta del backend
cd backend

# 2. Crea un entorno virtual
python -m venv .venv

# 3. Activa el entorno virtual
# En Windows (PowerShell):
.\.venv\Scripts\Activate.ps1
# En macOS/Linux:
source .venv/bin/activate

# 4. Instala las dependencias de Python
pip install -r requirements.txt

# 5. Configura tus credenciales
# Crea un archivo llamado .env en la carpeta 'backend' y aÃ±ade tus claves:
# SUPABASE_URL="TU_URL_DE_SUPABASE"
# SUPABASE_KEY="TU_ANON_KEY_DE_SUPABASE"
```

### 3. Configurar el Frontend

```bash
# 1. Desde la raÃ­z del proyecto, navega a la carpeta del frontend
cd ../frontend

# 2. Instala las dependencias de Node.js
npm install
```

---

## â–¶ï¸ CÃ³mo Ejecutar la AplicaciÃ³n

### 1. Iniciar el Servidor de Ollama

AsegÃºrate de que la aplicaciÃ³n de Ollama se estÃ© ejecutando en tu mÃ¡quina.

### 2. Iniciar el Backend (FastAPI)

Con el entorno virtual del backend activado (`.venv`), ejecuta el siguiente comando desde la carpeta `backend`:

```bash
uvicorn app.main:app --reload
```

El servidor backend estarÃ¡ disponible en `http://localhost:8000`. Puedes ver la documentaciÃ³n de la API en `http://localhost:8000/docs`.

### 3. Iniciar el Frontend (React)

En una **nueva terminal**, navega a la carpeta `frontend` y ejecuta:

```bash
npm start
```

La aplicaciÃ³n se abrirÃ¡ automÃ¡ticamente en tu navegador en `http://localhost:3000`.

Â¡Y listo! Ya puedes interactuar con el chatbot.

---

## ğŸ“‚ Estructura del Proyecto

```
.
â”œâ”€â”€ backend/            # CÃ³digo del servidor FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ .venv/
â”‚   â”œâ”€â”€ .env            # (No versionado) Credenciales
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/           # CÃ³digo de la aplicaciÃ³n React
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ .gitignore          # Archivos ignorados por Git
â””â”€â”€ README.md           # Este archivo
```
